{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Requests and BeautifulSoup\n",
    "\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites and converting it into a structured format (like CSV, Excel, or a database) so it can be analyzed or reused.\n",
    "\n",
    "Websites contain huge amounts of data, but most of it is:\n",
    "- Unstructured (HTML pages)\n",
    "- Not directly downloadable\n",
    "\n",
    "## Web Scraping Architecture / Working \n",
    "Steps:\n",
    "\n",
    "- Send HTTP request to server\n",
    "\n",
    "- Receive HTML response\n",
    "\n",
    "- Parse HTML (DOM tree)\n",
    "\n",
    "- Extract required elements\n",
    "\n",
    "- Store extracted data\n",
    "\n",
    "\n",
    "### HTTP Methods Used in Scraping\n",
    "\n",
    "| Method | Description            |\n",
    "| ------ | ---------------------- |\n",
    "| GET    | Retrieve web page data |\n",
    "| POST   | Send form data         |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Web Scraping\n",
    "\n",
    "- Static Web Scraping :\n",
    "    Static web scraping is the process of extracting data from websites where the content is directly available in the HTML source code returned by the server.\n",
    "    Tools like `request`, `BeautifulSoup`, `lxml` is used for scrapping.\n",
    "\n",
    "- Dynamic Web Scraping :\n",
    "    Dynamic web scraping extracts data from websites where the content is generated or loaded using JavaScript after the page loads. The data is not visible in the initial HTML source.\n",
    "    Tools like `Selenium` , `Playwright` is used for scrapping. Here, the webpage is simulate as user actions.\n",
    "\n",
    "- API-Based Scraping :\n",
    "    API-based scraping involves directly accessing data through official APIs provided by websites instead of scraping HTML pages.\n",
    "    Data is returned in structured formats such as JSON or XML.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Begin by importing the essential libraries.\n",
    "\n",
    "- `requests` is used for sending HTTP requests to fetch web pages. \n",
    "- `BeautifulSoup` parses the HTML response into a navigable structure. \n",
    "- We also import `time` for implementing delays, `csv` for data export, and `pandas` for advanced data manipulation and analysis of scraped content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in e:\\pyhton_lab\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: requests in e:\\pyhton_lab\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: lxml in e:\\pyhton_lab\\.venv\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: pandas in e:\\pyhton_lab\\.venv\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: numpy>=2.3.3 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in e:\\pyhton_lab\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests lxml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "- To start scraping, we need to fetch the webpage content using an HTTP GET request. \n",
    "- The `requests.get()` method retrieves the page. It's crucial to include headers like 'User-Agent' to simulate a browser request, as some sites block default Python agents. \n",
    "- Check the response status: 200 indicates success, while 4xx/5xx codes signal errors. \n",
    "- Handle exceptions to make your scraper robust against network issues or invalid URLs.\n",
    "\n",
    "Here, we use https://quotes.toscrape.com/ , a site explicitly designed for scraping practice, to demonstrate techniques safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request error: HTTPSConnectionPool(host='quotes.toscrape.com', port=443): Max retries exceeded with url: /page/1/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1081)')))\n"
     ]
    }
   ],
   "source": [
    "url = 'https://quotes.toscrape.com/page/1/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    print(f'Status Code: {response.status_code}')\n",
    "    print(f'Content Length: {len(response.text)} bytes')\n",
    "    print(f'Content Preview: {response.text[:300]}...')  # Extended preview for better inspection\n",
    "    \n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f'HTTP error occurred: {http_err}')\n",
    "    \n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f'Request error: {req_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "- With the raw HTML obtained, parse it using BeautifulSoup. \n",
    "- Choose a parser like 'html.parser' (built-in) or 'lxml' (faster, requires installation). The parsed 'soup' object allows tree-like navigation of the HTML DOM.\n",
    "\n",
    "Key navigation methods include:\n",
    "- `soup.find('tag', attrs={'class': 'classname'})`: Locates the first matching element.\n",
    "- `soup.find_all('tag', attrs={'id': 'idname'})`: Retrieves all matches as a list.\n",
    "- Access text with `.text.strip()` to clean whitespace.\n",
    "- Get attributes like hyperlinks with `element['href']`.\n",
    "\n",
    "Use browser developer tools (F12) to inspect elements and identify selectors (tags, classes, IDs). Here, we extract the page title and count quote elements for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m soup = BeautifulSoup(\u001b[43mresponse\u001b[49m.text, \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Using 'lxml' for efficiency\u001b[39;00m\n\u001b[32m      3\u001b[39m title = soup.find(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPage Title: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle.text.strip()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtitle\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTitle not found\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, 'lxml')  # Using 'lxml' for efficiency\n",
    "\n",
    "title = soup.find('title')\n",
    "print(f'Page Title: {title.text.strip() if title else \"Title not found\"}')\n",
    "\n",
    "quotes = soup.find_all('div', class_='quote')\n",
    "print(f'Number of Quotes on Page: {len(quotes)}')\n",
    "\n",
    "# For debugging: Pretty-print a sample element\n",
    "if quotes:\n",
    "    print('\\nSample Quote HTML:')\n",
    "    print(quotes[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :\n",
    "- Now, extract structured data from the parsed elements. \n",
    "- Loop through the quote divs to pull quotes, authors, and tags. Use conditional checks to handle missing elements gracefully, preventing crashes. Store data in a list of dictionaries for easy conversion to DataFrames or CSV.\n",
    "\n",
    "- Enhance extraction with list comprehensions for tags. This step demonstrates data cleaning, such as stripping quotes of surrounding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebElement' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m data = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m quote \u001b[38;5;129;01min\u001b[39;00m quotes:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     text_element = \u001b[43mquote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mspan\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     author_element = quote.find(\u001b[33m'\u001b[39m\u001b[33msmall\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     tags_elements = quote.find_all(\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'WebElement' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for quote in quotes:\n",
    "    text_element = quote.find('span', class_='text')\n",
    "    author_element = quote.find('small', class_='author')\n",
    "    tags_elements = quote.find_all('a', class_='tag')\n",
    "    \n",
    "    text = text_element.text.strip()[1:-1] if text_element else 'N/A'  # Remove surrounding quotes\n",
    "    author = author_element.text.strip() if author_element else 'N/A'\n",
    "    tags = [tag.text.strip() for tag in tags_elements] if tags_elements else []\n",
    "    author_link = author_element.find_next_sibling('a')['href'] if author_element and author_element.find_next_sibling('a') else 'N/A'\n",
    "    \n",
    "    data.append({\n",
    "        'quote': text,\n",
    "        'author': author,\n",
    "        'tags': ', '.join(tags),\n",
    "        'author_link': f'https://quotes.toscrape.com{author_link}' if author_link != 'N/A' else 'N/A'\n",
    "    })\n",
    "\n",
    "# Display extracted data as a DataFrame for review\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: \n",
    "- Websites often span multiple pages. Implement pagination by detecting 'next' links and iterating until none remain. \n",
    "- Incorporate `time.sleep()` for ethical scraping. This loop accumulates data across pages.\n",
    "\n",
    "Advanced: Use sessions (`requests.Session()`) for persistent connections and cookies. Monitor for infinite loops by setting a max page limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed page 1\n",
      "Processed page 2\n",
      "Processed page 3\n",
      "Processed page 4\n",
      "Processed page 5\n",
      "Processed page 6\n",
      "Processed page 7\n",
      "Processed page 8\n",
      "Processed page 9\n",
      "Processed page 10\n",
      "Total Quotes Scraped: 100\n",
      "                                                    quote           author  \\\n",
      "count                                                 100              100   \n",
      "unique                                                100               50   \n",
      "top     The world as we have created it is a process o...  Albert Einstein   \n",
      "freq                                                    1               10   \n",
      "\n",
      "        tags                                        author_link  \n",
      "count    100                                                100  \n",
      "unique    84                                                 50  \n",
      "top     love  https://quotes.toscrape.com/author/Albert-Eins...  \n",
      "freq       4                                                 10  \n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "current_url = f'{base_url}/page/1/'\n",
    "session = requests.Session()  # For efficiency in multiple requests\n",
    "max_pages = 10  # Safety limit\n",
    "page_count = 0\n",
    "\n",
    "while current_url and page_count < max_pages:\n",
    "    try:\n",
    "        response = session.get(current_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        quotes = soup.find_all('div', class_='quote')\n",
    "        for quote in quotes:\n",
    "            text = quote.find('span', class_='text').text.strip()[1:-1]\n",
    "            author = quote.find('small', class_='author').text.strip()\n",
    "            tags = ', '.join([tag.text.strip() for tag in quote.find_all('a', class_='tag')])\n",
    "            author_link = base_url + quote.find('a', href=True)['href'] if quote.find('a', href=True) else 'N/A'\n",
    "            all_data.append({'quote': text, 'author': author, 'tags': tags, 'author_link': author_link})\n",
    "        \n",
    "        next_button = soup.select_one('li.next > a')\n",
    "        next_link = next_button['href'] if next_button else None\n",
    "        current_url = base_url + next_link if next_link else None\n",
    "        \n",
    "        page_count += 1\n",
    "        print(f'Processed page {page_count}')\n",
    "        time.sleep(2)  # Increased delay for server courtesy\n",
    "    except Exception as e:\n",
    "        print(f'Error on page {page_count}: {e}')\n",
    "        break\n",
    "\n",
    "print(f'Total Quotes Scraped: {len(all_data)}')\n",
    "all_df = pd.DataFrame(all_data)\n",
    "print(all_df.describe())  # Statistical summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After scraping, save the data for persistence. \n",
    "- Use CSV for simple tabular data or JSON for nested structures. Here, we export to CSV with proper encoding to handle special characters. Optionally, use Pandas to export to Excel for richer formatting.\n",
    "\n",
    "- Validate the saved file by reloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = pd.DataFrame(\u001b[43mall_data\u001b[49m)\n\u001b[32m      2\u001b[39m data.to_csv(\u001b[33m'\u001b[39m\u001b[33mall_quotes.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mData exported to all_quotes.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(all_data)\n",
    "data.to_csv('all_quotes.csv', index=False, encoding='utf-8')\n",
    "print('Data exported to all_quotes.csv')\n",
    "\n",
    "# Verify by reading back\n",
    "verify_df = pd.read_csv('all_quotes.csv')\n",
    "print(verify_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For robustness, implement retry mechanisms for failed requests. Use proxies if scraping at scale to rotate IPs. Handle dynamic content: If JavaScript renders the page, consider alternatives like Selenium (for browser automation) or APIs if available.\n",
    "\n",
    "Ethical and Legal Notes:\n",
    "- Rate limit: 1-5 seconds per request.\n",
    "- User-Agent rotation: Vary headers to avoid detection.\n",
    "- Compliance: GDPR/CCPA for data privacy.\n",
    "- Alternatives: Prefer official APIs (e.g., Twitter API over scraping).\n",
    "\n",
    "Common Issues:\n",
    "- Encoding: Set `response.encoding = 'utf-8'`.\n",
    "- Selectors change: Use XPath or CSS selectors via `soup.select()`.\n",
    "- Anti-scraping: Captchas require OCR or services like 2Captcha.\n",
    "\n",
    "Example Retry Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry test successful\n"
     ]
    }
   ],
   "source": [
    "def get_with_retry(url, headers, retries=3, backoff=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Attempt {attempt + 1} failed: {e}')\n",
    "            time.sleep(backoff * (attempt + 1))\n",
    "    raise Exception('Max retries exceeded')\n",
    "\n",
    "# Test retry\n",
    "try:\n",
    "    test_response = get_with_retry('https://quotes.toscrape.com/page/1/', headers)\n",
    "    print('Retry test successful')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To further understanding, analyze the scraped data.\n",
    "- Use Pandas for grouping authors by quote count or tag frequency. Visualize with Matplotlib if installed.\n",
    "\n",
    "Experiment by adapting to other sites, always ethically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Authors by Quote Count:\n",
      "author\n",
      "Albert Einstein      10\n",
      "J.K. Rowling          9\n",
      "Marilyn Monroe        7\n",
      "Dr. Seuss             6\n",
      "Mark Twain            6\n",
      "Jane Austen           5\n",
      "C.S. Lewis            5\n",
      "Bob Marley            3\n",
      "Mother Teresa         2\n",
      "Eleanor Roosevelt     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top Tags:\n",
      "tags\n",
      "love                    14\n",
      "inspirational           13\n",
      "life                    13\n",
      "humor                   12\n",
      "books                   11\n",
      "reading                  7\n",
      "friendship               5\n",
      "friends                  4\n",
      "truth                    4\n",
      "attributed-no-source     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis Example\n",
    "author_counts = all_df['author'].value_counts()\n",
    "print('Top Authors by Quote Count:')\n",
    "print(author_counts.head(10))\n",
    "\n",
    "# Tag frequency (split and count)\n",
    "all_tags = all_df['tags'].str.split(', ').explode().value_counts()\n",
    "print('\\nTop Tags:')\n",
    "print(all_tags.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Web Scraping with Selenium\n",
    "\n",
    "Some websites load data dynamically using JavaScript (AJAX) after the page loads. The `requests` library only fetches the initial HTML, so it often misses this data.\n",
    "\n",
    "**Project Goal:** Scrape quotes from [http://quotes.toscrape.com/js/](http://quotes.toscrape.com/js/), where quotes are rendered by JavaScript.\n",
    "**Tools:** `Selenium` (for browser automation) and `webdriver-manager` (to handle driver installation).\n",
    "\n",
    "Note: This requires a web browser (like Chrome) to be installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.40.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: certifi>=2026.1.4 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from selenium) (2026.1.4)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting trio-typing>=0.10.0 (from selenium)\n",
      "  Downloading trio_typing-0.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting types-certifi>=2021.10.8.3 (from selenium)\n",
      "  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3>=1.26.25.14 (from selenium)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.6.3 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (2.6.3)\n",
      "Collecting websocket-client<2.0,>=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in e:\\pyhton_lab\\.venv\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in e:\\pyhton_lab\\.venv\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.6.3->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in e:\\pyhton_lab\\.venv\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: packaging in e:\\pyhton_lab\\.venv\\lib\\site-packages (from webdriver-manager) (26.0)\n",
      "Collecting pycparser (from cffi>=1.14->trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting mypy-extensions>=0.4.2 (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting async-generator (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting importlib-metadata (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting h11<1,>=0.16.0 (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata->trio-typing>=0.10.0->selenium)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Downloading selenium-4.40.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.6 MB 7.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.5/9.6 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.6 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.6 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 7.3 MB/s  0:00:04\n",
      "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl (185 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading trio_typing-0.10.0-py3-none-any.whl (42 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: types-urllib3, types-certifi, zipp, websocket-client, sniffio, python-dotenv, pysocks, pycparser, outcome, mypy-extensions, h11, async-generator, wsproto, webdriver-manager, importlib-metadata, cffi, trio, trio-websocket, trio-typing, selenium\n",
      "\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ------ ---------------------------------  3/20 [websocket-client]\n",
      "   ---------- -----------------------------  5/20 [python-dotenv]\n",
      "   ---------- -----------------------------  5/20 [python-dotenv]\n",
      "   -------------- -------------------------  7/20 [pycparser]\n",
      "   ---------------- -----------------------  8/20 [outcome]\n",
      "   -------------------- ------------------- 10/20 [h11]\n",
      "   -------------------- ------------------- 10/20 [h11]\n",
      "   ---------------------- ----------------- 11/20 [async-generator]\n",
      "   ------------------------ --------------- 12/20 [wsproto]\n",
      "   -------------------------- ------------- 13/20 [webdriver-manager]\n",
      "   -------------------------- ------------- 13/20 [webdriver-manager]\n",
      "   ---------------------------- ----------- 14/20 [importlib-metadata]\n",
      "   ---------------------------- ----------- 14/20 [importlib-metadata]\n",
      "   ------------------------------ --------- 15/20 [cffi]\n",
      "   ------------------------------ --------- 15/20 [cffi]\n",
      "   ------------------------------ --------- 15/20 [cffi]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   -------------------------------- ------- 16/20 [trio]\n",
      "   ---------------------------------- ----- 17/20 [trio-websocket]\n",
      "   ------------------------------------ --- 18/20 [trio-typing]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   -------------------------------------- - 19/20 [selenium]\n",
      "   ---------------------------------------- 20/20 [selenium]\n",
      "\n",
      "Successfully installed async-generator-1.10 cffi-2.0.0 h11-0.16.0 importlib-metadata-8.7.1 mypy-extensions-1.1.0 outcome-1.3.0.post0 pycparser-3.0 pysocks-1.7.1 python-dotenv-1.2.1 selenium-4.40.0 sniffio-1.3.1 trio-0.32.0 trio-typing-0.10.0 trio-websocket-0.12.2 types-certifi-2021.10.8.3 types-urllib3-1.26.25.14 webdriver-manager-4.0.2 websocket-client-1.9.0 wsproto-1.3.2 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "# Install Selenium and helper for driver management\n",
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Browser...\n",
      "Navigating to http://quotes.toscrape.com/js/...\n",
      "Successfully loaded 10 quotes via JavaScript.\n",
      "\n",
      "Browser closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The world as we have created it is a process ...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>change, deep-thoughts, thinking, world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“It is our choices, Harry, that show what we t...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>abilities, choices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“There are only two ways to live your life. On...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>inspirational, life, live, miracle, miracles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>aliteracy, books, classic, humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Imperfection is beauty, madness is genius and...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>be-yourself, inspirational</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote           author  \\\n",
       "0  “The world as we have created it is a process ...  Albert Einstein   \n",
       "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
       "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
       "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
       "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
       "\n",
       "                                           tags  \n",
       "0        change, deep-thoughts, thinking, world  \n",
       "1                            abilities, choices  \n",
       "2  inspirational, life, live, miracle, miracles  \n",
       "3              aliteracy, books, classic, humor  \n",
       "4                    be-yourself, inspirational  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup the Driver (Headless mode for running without UI)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless') \n",
    "# options.add_argument('--start-maximized') # Use this if you want to see the browser\n",
    "\n",
    "print(\"Initializing Browser...\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "selenium_data = []\n",
    "\n",
    "try:\n",
    "    # 2. Navigate to the JS-rendered page\n",
    "    url = \"http://quotes.toscrape.com/js/\"\n",
    "    print(f\"Navigating to {url}...\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # 3. Wait for the dynamic content to load\n",
    "    # We wait up to 10 seconds for elements with class 'quote' to appear\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    quotes = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n",
    "    \n",
    "    print(f\"Successfully loaded {len(quotes)} quotes via JavaScript.\\n\")\n",
    "\n",
    "    # 4. Extract Data\n",
    "    for quote in quotes:\n",
    "        text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "        author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "        \n",
    "        # Tags are also inside, let's grab them\n",
    "        tags_elements = quote.find_elements(By.CLASS_NAME, \"tag\")\n",
    "        tags = [t.text for t in tags_elements]\n",
    "        \n",
    "        selenium_data.append({\n",
    "            \"quote\": text,\n",
    "            \"author\": author,\n",
    "            \"tags\": \", \".join(tags)\n",
    "        })\n",
    "\n",
    "finally:\n",
    "    # 5. Always close the driver\n",
    "    driver.quit()\n",
    "    print(\"Browser closed.\")\n",
    "\n",
    "# Display results\n",
    "selenium_df = pd.DataFrame(selenium_data)\n",
    "selenium_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Make it Fun! Visualizing & Interacting with Data\n",
    "\n",
    "Data scraping is just the beginning. The real fun starts when you play with the data you've collected. Let's add some visuals and a game to make this notebook more interactive!\n",
    "\n",
    "### 1. Word Cloud Visualization\n",
    "A Word Cloud represents the frequency of words visually. The bigger the word, the more often it appears in our tags.\n",
    "\n",
    "**Note:** You might need to install `wordcloud` and `matplotlib` if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.6-cp314-cp314-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp314-cp314-win_amd64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from wordcloud) (2.4.1)\n",
      "Requirement already satisfied: pillow in e:\\pyhton_lab\\.venv\\lib\\site-packages (from wordcloud) (12.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp314-cp314-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp314-cp314-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from matplotlib) (26.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\pyhton_lab\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading wordcloud-1.9.6-cp314-cp314-win_amd64.whl (308 kB)\n",
      "Downloading matplotlib-3.10.8-cp314-cp314-win_amd64.whl (8.3 MB)\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.3 MB 6.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.8/8.3 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.7/8.3 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.6/8.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.3/8.3 MB 6.6 MB/s  0:00:04\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp314-cp314-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.1/2.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 8.6 MB/s  0:00:01\n",
      "Downloading kiwisolver-1.4.9-cp314-cp314-win_amd64.whl (75 kB)\n",
      "Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, matplotlib, wordcloud\n",
      "\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   -------------------------- ------------- 4/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [wordcloud]\n",
      "   --------------------------------- ------ 5/6 [wordcloud]\n",
      "   ---------------------------------------- 6/6 [wordcloud]\n",
      "\n",
      "Successfully installed cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.2 wordcloud-1.9.6\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Combine all tags into a single string\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(tag \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_df\u001b[49m.tags \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tag, \u001b[38;5;28mstr\u001b[39m))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create the wordcloud object\u001b[39;00m\n\u001b[32m      8\u001b[39m wordcloud = WordCloud(width=\u001b[32m800\u001b[39m, height=\u001b[32m400\u001b[39m, background_color=\u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m, colormap=\u001b[33m'\u001b[39m\u001b[33mviridis\u001b[39m\u001b[33m'\u001b[39m).generate(text)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Beginner Tip: We need the data from the previous steps!\n",
    "# This check handles the case where you might have run this cell without running the scraping code first.\n",
    "if 'all_df' not in locals():\n",
    "    if os.path.exists('all_quotes.csv'):\n",
    "        print(\"Variable 'all_df' not found. Loading from 'all_quotes.csv'...\")\n",
    "        all_df = pd.read_csv('all_quotes.csv')\n",
    "    else:\n",
    "        print(\"❌ Error: No data found!\")\n",
    "        print(\"Please run 'Step 5' (Scraping Loop) in this notebook first to collect the data.\")\n",
    "        # Create a dummy df so the code below doesn't crash immediately with NameError\n",
    "        all_df = pd.DataFrame(columns=['tags'])\n",
    "\n",
    "if not all_df.empty and 'tags' in all_df.columns:\n",
    "    # Combine all tags into a single string\n",
    "    # We use str(tag) to handle potential numbers or missing values gracefully\n",
    "    text = \" \".join(str(tag) for tag in all_df['tags'].dropna())\n",
    "\n",
    "    if text.strip():\n",
    "        # Create the wordcloud object\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "\n",
    "        # Display the generated image:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Most Popular Quote Tags\", fontsize=20)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ No tags were found to visualize.\")\n",
    "else:\n",
    "    print(\"⚠️ DataFrame is empty or missing 'tags' column. Did the scraping finish successfully?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mini-Game: Who Said It?\n",
    "Test your knowledge (or luck!) with a simple interactive quiz based on the data you scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⚠️ Invalid input. Game over!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run the game\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mstart_quiz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mstart_quiz\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_quiz\u001b[39m():\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Pick a random row from our dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     random_row = \u001b[43mall_df\u001b[49m.sample(\u001b[32m1\u001b[39m).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m     quote = random_row[\u001b[33m'\u001b[39m\u001b[33mquote\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m     correct_author = random_row[\u001b[33m'\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'all_df' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def start_quiz():\n",
    "    # Pick a random row from our dataframe\n",
    "    random_row = all_df.sample(1).iloc[0]\n",
    "    quote = random_row['quote']\n",
    "    correct_author = random_row['author']\n",
    "    \n",
    "    # Get 3 other random authors for multiple choice\n",
    "    distractors = all_df[all_df['author'] != correct_author]['author'].sample(3).tolist()\n",
    "    options = distractors + [correct_author]\n",
    "    random.shuffle(options)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🕵️  MYSTERY QUOTE  🕵️\")\n",
    "    print(\"=\"*50)\n",
    "    print(f'\"{quote}\"')\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Who said this?\")\n",
    "    \n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{i+1}. {option}\")\n",
    "        \n",
    "    try:\n",
    "        user_choice = int(input(\"\\nEnter the number of your guess: \"))\n",
    "        if options[user_choice-1] == correct_author:\n",
    "            print(\"\\n✅ Correct! You're a pro!\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Oops! It was actually {correct_author}.\")\n",
    "    except (ValueError, IndexError):\n",
    "        print(\"\\n⚠️ Invalid input. Game over!\")\n",
    "\n",
    "# Run the game\n",
    "start_quiz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
