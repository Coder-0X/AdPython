{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping using Requests and BeautifulSoup\n",
        "\n",
        "\n",
        "Web scraping is the process of automatically extracting data from websites and converting it into a structured format (like CSV, Excel, or a database) so it can be analyzed or reused.\n",
        "\n",
        "Websites contain huge amounts of data, but most of it is:\n",
        "- Unstructured (HTML pages)\n",
        "- Not directly downloadable\n",
        "\n",
        "## Web Scraping Architecture / Working \n",
        "Steps:\n",
        "\n",
        "- Send HTTP request to server\n",
        "\n",
        "- Receive HTML response\n",
        "\n",
        "- Parse HTML (DOM tree)\n",
        "\n",
        "- Extract required elements\n",
        "\n",
        "- Store extracted data\n",
        "\n",
        "\n",
        "### HTTP Methods Used in Scraping\n",
        "\n",
        "| Method | Description            |\n",
        "| ------ | ---------------------- |\n",
        "| GET    | Retrieve web page data |\n",
        "| POST   | Send form data         |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Web Scraping\n",
        "\n",
        "- Static Web Scraping :\n",
        "    Static web scraping is the process of extracting data from websites where the content is directly available in the HTML source code returned by the server.\n",
        "    Tools like `request`, `BeautifulSoup`, `lxml` is used for scrapping.\n",
        "\n",
        "- Dynamic Web Scraping :\n",
        "    Dynamic web scraping extracts data from websites where the content is generated or loaded using JavaScript after the page loads. The data is not visible in the initial HTML source.\n",
        "    Tools like `Selenium` , `Playwright` is used for scrapping. Here, the webpage is simulate as user actions.\n",
        "\n",
        "- API-Based Scraping :\n",
        "    API-based scraping involves directly accessing data through official APIs provided by websites instead of scraping HTML pages.\n",
        "    Data is returned in structured formats such as JSON or XML.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1:\n",
        "Begin by importing the essential libraries.\n",
        "\n",
        "- `requests` is used for sending HTTP requests to fetch web pages. \n",
        "- `BeautifulSoup` parses the HTML response into a navigable structure. \n",
        "- We also import `time` for implementing delays, `csv` for data export, and `pandas` for advanced data manipulation and analysis of scraped content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2:\n",
        "- To start scraping, we need to fetch the webpage content using an HTTP GET request. \n",
        "- The `requests.get()` method retrieves the page. It's crucial to include headers like 'User-Agent' to simulate a browser request, as some sites block default Python agents. \n",
        "- Check the response status: 200 indicates success, while 4xx/5xx codes signal errors. \n",
        "- Handle exceptions to make your scraper robust against network issues or invalid URLs.\n",
        "\n",
        "Here, we use https://quotes.toscrape.com/ , a site explicitly designed for scraping practice, to demonstrate techniques safely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 200\n",
            "Content Length: 11021 bytes\n",
            "Content Preview: <!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "\t<meta charset=\"UTF-8\">\n",
            "\t<title>Quotes to Scrape</title>\n",
            "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
            "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
            "    \n",
            "    \n",
            "</head>\n",
            "<body>\n",
            "    <div class=\"container\">\n",
            "        <div class=\"row header-box\">\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "url = 'https://quotes.toscrape.com/page/1/'\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    print(f'Status Code: {response.status_code}')\n",
        "    print(f'Content Length: {len(response.text)} bytes')\n",
        "    print(f'Content Preview: {response.text[:300]}...')  # Extended preview for better inspection\n",
        "    \n",
        "except requests.exceptions.HTTPError as http_err:\n",
        "    print(f'HTTP error occurred: {http_err}')\n",
        "    \n",
        "except requests.exceptions.RequestException as req_err:\n",
        "    print(f'Request error: {req_err}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3:\n",
        "- With the raw HTML obtained, parse it using BeautifulSoup. \n",
        "- Choose a parser like 'html.parser' (built-in) or 'lxml' (faster, requires installation). The parsed 'soup' object allows tree-like navigation of the HTML DOM.\n",
        "\n",
        "Key navigation methods include:\n",
        "- `soup.find('tag', attrs={'class': 'classname'})`: Locates the first matching element.\n",
        "- `soup.find_all('tag', attrs={'id': 'idname'})`: Retrieves all matches as a list.\n",
        "- Access text with `.text.strip()` to clean whitespace.\n",
        "- Get attributes like hyperlinks with `element['href']`.\n",
        "\n",
        "Use browser developer tools (F12) to inspect elements and identify selectors (tags, classes, IDs). Here, we extract the page title and count quote elements for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Page Title: Quotes to Scrape\n",
            "Number of Quotes on Page: 10\n",
            "\n",
            "Sample Quote HTML:\n",
            "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
            " <span class=\"text\" itemprop=\"text\">\n",
            "  “The truth.\" Dumbledore sighed. \"It is a beautiful and terrible thing, and should therefore be treated with great caution.”\n",
            " </span>\n",
            " <span>\n",
            "  by\n",
            "  <small class=\"author\" itemprop=\"author\">\n",
            "   J.K. Rowling\n",
            "  </small>\n",
            "  <a href=\"/author/J-K-Rowling\">\n",
            "   (about)\n",
            "  </a>\n",
            " </span>\n",
            " <div class=\"tags\">\n",
            "  Tags:\n",
            "  <meta class=\"keywords\" content=\"truth\" itemprop=\"keywords\"/>\n",
            "  <a class=\"tag\" href=\"/tag/truth/page/1/\">\n",
            "   truth\n",
            "  </a>\n",
            " </div>\n",
            "</div>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "soup = BeautifulSoup(response.text, 'lxml')  # Using 'lxml' for efficiency\n",
        "\n",
        "title = soup.find('title')\n",
        "print(f'Page Title: {title.text.strip() if title else \"Title not found\"}')\n",
        "\n",
        "quotes = soup.find_all('div', class_='quote')\n",
        "print(f'Number of Quotes on Page: {len(quotes)}')\n",
        "\n",
        "# For debugging: Pretty-print a sample element\n",
        "if quotes:\n",
        "    print('\\nSample Quote HTML:')\n",
        "    print(quotes[0].prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 :\n",
        "- Now, extract structured data from the parsed elements. \n",
        "- Loop through the quote divs to pull quotes, authors, and tags. Use conditional checks to handle missing elements gracefully, preventing crashes. Store data in a list of dictionaries for easy conversion to DataFrames or CSV.\n",
        "\n",
        "- Enhance extraction with list comprehensions for tags. This step demonstrates data cleaning, such as stripping quotes of surrounding characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               quote           author  \\\n",
            "0  The truth.\" Dumbledore sighed. \"It is a beauti...     J.K. Rowling   \n",
            "1  I'm the one that's got to die when it's time f...     Jimi Hendrix   \n",
            "2           To die will be an awfully big adventure.      J.M. Barrie   \n",
            "3  It takes courage to grow up and become who you...    E.E. Cummings   \n",
            "4  But better to get hurt by the truth than comfo...  Khaled Hosseini   \n",
            "\n",
            "              tags                                        author_link  \n",
            "0            truth     https://quotes.toscrape.com/author/J-K-Rowling  \n",
            "1      death, life    https://quotes.toscrape.com/author/Jimi-Hendrix  \n",
            "2  adventure, love      https://quotes.toscrape.com/author/J-M-Barrie  \n",
            "3          courage    https://quotes.toscrape.com/author/E-E-Cummings  \n",
            "4             life  https://quotes.toscrape.com/author/Khaled-Hoss...  \n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "\n",
        "for quote in quotes:\n",
        "    text_element = quote.find('span', class_='text')\n",
        "    author_element = quote.find('small', class_='author')\n",
        "    tags_elements = quote.find_all('a', class_='tag')\n",
        "    \n",
        "    text = text_element.text.strip()[1:-1] if text_element else 'N/A'  # Remove surrounding quotes\n",
        "    author = author_element.text.strip() if author_element else 'N/A'\n",
        "    tags = [tag.text.strip() for tag in tags_elements] if tags_elements else []\n",
        "    author_link = author_element.find_next_sibling('a')['href'] if author_element and author_element.find_next_sibling('a') else 'N/A'\n",
        "    \n",
        "    data.append({\n",
        "        'quote': text,\n",
        "        'author': author,\n",
        "        'tags': ', '.join(tags),\n",
        "        'author_link': f'https://quotes.toscrape.com{author_link}' if author_link != 'N/A' else 'N/A'\n",
        "    })\n",
        "\n",
        "# Display extracted data as a DataFrame for review\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: \n",
        "- Websites often span multiple pages. Implement pagination by detecting 'next' links and iterating until none remain. \n",
        "- Incorporate `time.sleep()` for ethical scraping. This loop accumulates data across pages.\n",
        "\n",
        "Advanced: Use sessions (`requests.Session()`) for persistent connections and cookies. Monitor for infinite loops by setting a max page limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed page 1\n",
            "Processed page 2\n",
            "Processed page 3\n",
            "Processed page 4\n",
            "Processed page 5\n",
            "Processed page 6\n",
            "Processed page 7\n",
            "Processed page 8\n",
            "Processed page 9\n",
            "Processed page 10\n",
            "Total Quotes Scraped: 100\n",
            "                                                    quote           author  \\\n",
            "count                                                 100              100   \n",
            "unique                                                100               50   \n",
            "top     The world as we have created it is a process o...  Albert Einstein   \n",
            "freq                                                    1               10   \n",
            "\n",
            "        tags                                        author_link  \n",
            "count    100                                                100  \n",
            "unique    84                                                 50  \n",
            "top     love  https://quotes.toscrape.com/author/Albert-Eins...  \n",
            "freq       4                                                 10  \n"
          ]
        }
      ],
      "source": [
        "all_data = []\n",
        "base_url = '    '\n",
        "current_url = f'{base_url}/page/1/'\n",
        "session = requests.Session()  # For efficiency in multiple requests\n",
        "max_pages = 10  # Safety limit\n",
        "page_count = 0\n",
        "\n",
        "while current_url and page_count < max_pages:\n",
        "    try:\n",
        "        response = session.get(current_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "        \n",
        "        quotes = soup.find_all('div', class_='quote')\n",
        "        for quote in quotes:\n",
        "            text = quote.find('span', class_='text').text.strip()[1:-1]\n",
        "            author = quote.find('small', class_='author').text.strip()\n",
        "            tags = ', '.join([tag.text.strip() for tag in quote.find_all('a', class_='tag')])\n",
        "            author_link = base_url + quote.find('a', href=True)['href'] if quote.find('a', href=True) else 'N/A'\n",
        "            all_data.append({'quote': text, 'author': author, 'tags': tags, 'author_link': author_link})\n",
        "        \n",
        "        next_button = soup.select_one('li.next > a')\n",
        "        next_link = next_button['href'] if next_button else None\n",
        "        current_url = base_url + next_link if next_link else None\n",
        "        \n",
        "        page_count += 1\n",
        "        print(f'Processed page {page_count}')\n",
        "        time.sleep(2)  # Increased delay for server courtesy\n",
        "    except Exception as e:\n",
        "        print(f'Error on page {page_count}: {e}')\n",
        "        break\n",
        "\n",
        "print(f'Total Quotes Scraped: {len(all_data)}')\n",
        "all_df = pd.DataFrame(all_data)\n",
        "print(all_df.describe())  # Statistical summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- After scraping, save the data for persistence. \n",
        "- Use CSV for simple tabular data or JSON for nested structures. Here, we export to CSV with proper encoding to handle special characters. Optionally, use Pandas to export to Excel for richer formatting.\n",
        "\n",
        "- Validate the saved file by reloading it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data exported to all_quotes.csv\n",
            "                                               quote           author  \\\n",
            "0  The world as we have created it is a process o...  Albert Einstein   \n",
            "1  It is our choices, Harry, that show what we tr...     J.K. Rowling   \n",
            "2  There are only two ways to live your life. One...  Albert Einstein   \n",
            "3  The person, be it gentleman or lady, who has n...      Jane Austen   \n",
            "4  Imperfection is beauty, madness is genius and ...   Marilyn Monroe   \n",
            "\n",
            "                                           tags  \\\n",
            "0        change, deep-thoughts, thinking, world   \n",
            "1                            abilities, choices   \n",
            "2  inspirational, life, live, miracle, miracles   \n",
            "3              aliteracy, books, classic, humor   \n",
            "4                    be-yourself, inspirational   \n",
            "\n",
            "                                         author_link  \n",
            "0  https://quotes.toscrape.com/author/Albert-Eins...  \n",
            "1     https://quotes.toscrape.com/author/J-K-Rowling  \n",
            "2  https://quotes.toscrape.com/author/Albert-Eins...  \n",
            "3     https://quotes.toscrape.com/author/Jane-Austen  \n",
            "4  https://quotes.toscrape.com/author/Marilyn-Monroe  \n"
          ]
        }
      ],
      "source": [
        "data = pd.DataFrame(all_data)\n",
        "data.to_csv('all_quotes.csv', index=False, encoding='utf-8')\n",
        "print('Data exported to all_quotes.csv')\n",
        "\n",
        "# Verify by reading back\n",
        "verify_df = pd.read_csv('all_quotes.csv')\n",
        "print(verify_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For robustness, implement retry mechanisms for failed requests. Use proxies if scraping at scale to rotate IPs. Handle dynamic content: If JavaScript renders the page, consider alternatives like Selenium (for browser automation) or APIs if available.\n",
        "\n",
        "Ethical and Legal Notes:\n",
        "- Rate limit: 1-5 seconds per request.\n",
        "- User-Agent rotation: Vary headers to avoid detection.\n",
        "- Compliance: GDPR/CCPA for data privacy.\n",
        "- Alternatives: Prefer official APIs (e.g., Twitter API over scraping).\n",
        "\n",
        "Common Issues:\n",
        "- Encoding: Set `response.encoding = 'utf-8'`.\n",
        "- Selectors change: Use XPath or CSS selectors via `soup.select()`.\n",
        "- Anti-scraping: Captchas require OCR or services like 2Captcha.\n",
        "\n",
        "Example Retry Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retry test successful\n"
          ]
        }
      ],
      "source": [
        "def get_with_retry(url, headers, retries=3, backoff=2):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f'Attempt {attempt + 1} failed: {e}')\n",
        "            time.sleep(backoff * (attempt + 1))\n",
        "    raise Exception('Max retries exceeded')\n",
        "\n",
        "# Test retry\n",
        "try:\n",
        "    test_response = get_with_retry('https://quotes.toscrape.com/page/1/', headers)\n",
        "    print('Retry test successful')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- To further understanding, analyze the scraped data.\n",
        "- Use Pandas for grouping authors by quote count or tag frequency. Visualize with Matplotlib if installed.\n",
        "\n",
        "Experiment by adapting to other sites, always ethically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Authors by Quote Count:\n",
            "author\n",
            "Albert Einstein      10\n",
            "J.K. Rowling          9\n",
            "Marilyn Monroe        7\n",
            "Dr. Seuss             6\n",
            "Mark Twain            6\n",
            "Jane Austen           5\n",
            "C.S. Lewis            5\n",
            "Bob Marley            3\n",
            "Mother Teresa         2\n",
            "Eleanor Roosevelt     2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top Tags:\n",
            "tags\n",
            "love                    14\n",
            "inspirational           13\n",
            "life                    13\n",
            "humor                   12\n",
            "books                   11\n",
            "reading                  7\n",
            "friendship               5\n",
            "friends                  4\n",
            "truth                    4\n",
            "attributed-no-source     3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Data Analysis Example\n",
        "author_counts = all_df['author'].value_counts()\n",
        "print('Top Authors by Quote Count:')\n",
        "print(author_counts.head(10))\n",
        "\n",
        "# Tag frequency (split and count)\n",
        "all_tags = all_df['tags'].str.split(', ').explode().value_counts()\n",
        "print('\\nTop Tags:')\n",
        "print(all_tags.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
