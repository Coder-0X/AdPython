{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Structured/Unstructured Data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data\n",
    "\n",
    "Structured data refers to data that is organized in a predefined format, usually in the form of rows and columns.\n",
    "\n",
    "It follows a fixed schema, meaning the structure of the data is defined before data is stored.\n",
    "\n",
    "Because of this well-defined organization, structured data is easy to store, search, query, and analyze using traditional databases and data processing tools. \n",
    "\n",
    "Common examples include data stored in relational databases, spreadsheets, and CSV files, such as student records, transaction logs, and inventory tables.\n",
    "\n",
    "---\n",
    "\n",
    "## Unstructured Data\n",
    "\n",
    "Unstructured data refers to data that does not have a predefined format or fixed schema. \n",
    "\n",
    "It is not organized in a tabular form and cannot be easily stored or queried using traditional database systems.\n",
    "\n",
    "This type of data often contains text, images, audio, or video and requires preprocessing or advanced techniques such as text mining, natural language processing, or machine learning for analysis. \n",
    "\n",
    "Examples of unstructured data include emails, social media posts, documents, images, and multimedia files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CSV (Comma-Separated Values) - Structured Data\n",
    "\n",
    "CSV is a common format for tabular data. It's structured with rows and columns separated by commas (or other delimiters).\n",
    "\n",
    "Because of its simple and lightweight structure, CSV is widely supported by databases, spreadsheet applications, and programming languages. \n",
    "\n",
    "\n",
    "### Reading CSV\n",
    "- Using pandas `pd.read_csv()` to load CSV into a DataFrame.\n",
    "- Options: delimiter, header, index_col, dtype, etc.\n",
    "- or Using csv module\n",
    "\n",
    "### Writing CSV\n",
    "- Use `df.to_csv()` to save a DataFrame to CSV.\n",
    "- Options: index, header, sep, mode, etc.\n",
    "- or Using csv module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file written to: data/example.csv\n"
     ]
    }
   ],
   "source": [
    "# Example data for demonstration\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Writing to CSV using pandas\n",
    "csv_file = 'data/example.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f'CSV file written to: {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV Data:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "\n",
      "Read with options:\n",
      "          Age         City\n",
      "Name                      \n",
      "Alice    25.0     New York\n",
      "Bob      30.0  Los Angeles\n",
      "Charlie  35.0      Chicago\n"
     ]
    }
   ],
   "source": [
    "# Reading from CSV using pandas\n",
    "df_read = pd.read_csv(csv_file)\n",
    "print('Read CSV Data:')\n",
    "print(df_read)\n",
    "\n",
    "# Detailed options example\n",
    "df_read_with_options = pd.read_csv(csv_file, dtype={'Age': 'float'}, index_col='Name')\n",
    "print('\\nRead with options:')\n",
    "print(df_read_with_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file written using csv module to: data/example_module.csv\n",
      "['Name', 'Age', 'City']\n",
      "['Alice', '25', 'New York']\n",
      "['Bob', '30', 'Los Angeles']\n",
      "['Charlie', '35', 'Chicago']\n"
     ]
    }
   ],
   "source": [
    "# using csv module\n",
    "\n",
    "import csv\n",
    "# Writing to CSV using csv module\n",
    "csv_file_module = 'data/example_module.csv'\n",
    "with open(csv_file_module, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(data.keys())\n",
    "    writer.writerows(zip(*data.values()))\n",
    "print(f'CSV file written using csv module to: {csv_file_module}')\n",
    "\n",
    "# Reading from CSV using csv module\n",
    "with open(csv_file_module, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Deep Dive: How CSV is Handled in Memory\n",
    "\n",
    "When you execute `pd.read_csv('data/example.csv')`, the following occurs at the system level:\n",
    "\n",
    "1.  **Buffering & Tokenization**: \n",
    "    - The OS reads the file from the disk in chunks (typically 4KB or 8KB pages) into a memory buffer.\n",
    "    - The Pandas C-engine (written in C for speed) iterates over this byte stream, identifying delimiters (`,`) and newlines (`\\n`) to tokenize the data.\n",
    "\n",
    "2.  **Type Inference**:\n",
    "    - Pandas samples the first N rows (or the whole file if small) to infer data types.\n",
    "    - It allocates memory for **NumPy arrays**.\n",
    "    - **Integers/Floats**: Stored efficiently in contiguous blocks of memory (e.g., `int64` takes 8 bytes per value).\n",
    "    - **Strings/Objects**: Stored as an array of pointers to Python objects. This is much less memory-efficient than numbers.\n",
    "\n",
    "3.  **The Block Manager**:\n",
    "    - Pandas does not store columns purely individually. It groups columns of the same type into **Blocks**.\n",
    "    - If you have 3 Integer columns, they might be stored as a single `(3, N)` NumPy array matrix. This improves CPU cache locality.\n",
    "\n",
    "*Key Takeaway*: CSVs are text-heavy. The in-memory DataFrame representation is often **larger** than the file size on disk because ASCII text is converted into heavy Python objects or fixed-width NumPy types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JSON (JavaScript Object Notation) - Structured/Semi-Structured Data\n",
    "\n",
    "JSON is a lightweight format for structured data, often used in APIs and configurations.\n",
    "\n",
    "It represents data in keyâ€“value pairs and supports nested structures such as objects and arrays, making it flexible and easy to read by both humans and machines.\n",
    "\n",
    "### Reading JSON\n",
    "- Use `json.load()` for files or `json.loads()` for strings.\n",
    "- pandas `pd.read_json()` for DataFrame conversion.\n",
    "\n",
    "### Writing JSON\n",
    "- Use `json.dump()` or `json.dumps()`.\n",
    "- pandas `df.to_json()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file written to: data/example.json\n"
     ]
    }
   ],
   "source": [
    "# Writing to JSON using pandas\n",
    "json_file = 'data/example.json'\n",
    "df.to_json(json_file, orient='records', indent=4)\n",
    "print(f'JSON file written to: {json_file}')\n",
    "\n",
    "# Using built-in json\n",
    "with open('data/example_builtin.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read JSON Data (pandas):\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "\n",
      "Read JSON Data (built-in):\n",
      "[{'Name': 'Alice', 'Age': 25, 'City': 'New York'}, {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'}, {'Name': 'Charlie', 'Age': 35, 'City': 'Chicago'}]\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON with pandas\n",
    "df_json = pd.read_json(json_file, orient='records')\n",
    "print('Read JSON Data (pandas):')\n",
    "print(df_json)\n",
    "\n",
    "# Reading with built-in json\n",
    "with open(json_file, 'r') as f:\n",
    "    data_loaded = json.load(f)\n",
    "print('\\nRead JSON Data (built-in):')\n",
    "print(data_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Nested JSON\n",
    "- Nested JSON is very common in APIs, logs, and web data, where values can be dictionaries inside dictionaries or lists.\n",
    "- Use `json_normalize` in pandas for flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Nested JSON:\n",
      "    name  details.age details.city\n",
      "0  Alice           25     New York\n",
      "1    Bob           30  Los Angeles\n"
     ]
    }
   ],
   "source": [
    "nested_data = {\n",
    "    'employees': [\n",
    "        {'name': 'Alice', 'details': {'age': 25, 'city': 'New York'}},\n",
    "        {'name': 'Bob', 'details': {'age': 30, 'city': 'Los Angeles'}}\n",
    "    ]\n",
    "\n",
    "}\n",
    "with open('data/nested.json', 'w') as f:\n",
    "    json.dump(nested_data, f)\n",
    "\n",
    "df_nested = pd.json_normalize(nested_data['employees'])\n",
    "print('Normalized Nested JSON:')\n",
    "print(df_nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Deep Dive: JSON in Memory\n",
    "\n",
    "1.  **Parsing (DOM style)**:\n",
    "    - Standard `json.load()` reads the entire file content into a string and then parses it into a standard Python `dict` or `list`.\n",
    "    - **Hash Maps**: Python dictionaries are implemented as Hash Tables. They are fast for lookups (O(1)) but memory-heavy because they must store hash values, pointers to keys, pointers to values, and maintain empty space (sparsity) to avoid collisions.\n",
    "\n",
    "2.  **Pandas & Normalization**:\n",
    "    - When using `pd.read_json` or `json_normalize`, the library must traverse this hierarchical tree structure.\n",
    "    - It flattens the nested dictionaries into specific columns.\n",
    "    - Missing data in sparse JSONs results in `NaN` (Not a Number) values in the DataFrame, which are typically standard floating-point markers, consuming space even for \"empty\" cells.\n",
    "\n",
    "3.  **Overhead**:\n",
    "    - JSON is repetitive (keys are repeated for every record). While the file on disk is text, the in-memory Python object removes this textual repetition (keys are stored once in the string interning pool if strictly reused), but the dictionary overhead usually results in **2x-10x** memory expansion compared to raw bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Excel (XLSX) - Structured Data\n",
    "\n",
    "Excel files (.xlsx) are one of the most common structured data formats used in data analysis, business reporting, and research.\n",
    "\n",
    "Excel files handle spreadsheets with multiple sheets.\n",
    "\n",
    "### Reading Excel\n",
    "- Use `pd.read_excel()`.\n",
    "- Options: sheet_name, header, usecols, etc.\n",
    "\n",
    "### Writing Excel\n",
    "- Use `df.to_excel()`.\n",
    "- For multiple sheets, use `ExcelWriter`.\n",
    "\n",
    "\n",
    "openpyxl library for Excel (optional, but recommended for writing Excel files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in e:\\pyhton_lab\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in e:\\pyhton_lab\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file written to: data/example.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Writing to Excel\n",
    "excel_file = 'data/example.xlsx'\n",
    "df.to_excel(excel_file, index=False, sheet_name='Sheet1')\n",
    "print(f'Excel file written to: {excel_file}')\n",
    "\n",
    "# Writing multiple sheets\n",
    "with pd.ExcelWriter('data/multi_sheet.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    df.to_excel(writer, sheet_name='Sheet2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Excel Data:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "\n",
      "Read specific columns:\n",
      "      Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "\n",
      "Sheets:\n",
      "Sheet1:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "Sheet2:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n"
     ]
    }
   ],
   "source": [
    "# Reading Excel\n",
    "df_excel = pd.read_excel(excel_file, sheet_name='Sheet1')\n",
    "print('Read Excel Data:')\n",
    "print(df_excel)\n",
    "\n",
    "# Reading specific columns\n",
    "df_cols = pd.read_excel(excel_file, usecols=['Name', 'Age'])\n",
    "print('\\nRead specific columns:')\n",
    "print(df_cols)\n",
    "\n",
    "# Reading multiple sheets\n",
    "dfs = pd.read_excel('data/multi_sheet.xlsx', sheet_name=None)\n",
    "print('\\nSheets:')\n",
    "for sheet, df_sheet in dfs.items():\n",
    "    print(f'{sheet}:\\n{df_sheet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Deep Dive: Excel Memory Overhead\n",
    "\n",
    "Processing Excel files (`.xlsx`) is significantly more resource-intensive than CSV or JSON.\n",
    "\n",
    "1.  **Zip Decompression**:\n",
    "    - An `.xlsx` file is actually a zipped archive of multiple XML files.\n",
    "    - `read_excel` (via `openpyxl`) must first unzip these files into memory or temporary storage.\n",
    "\n",
    "2.  **XML Parsing**:\n",
    "    - The data in Excel is stored in XML format. The parser must traverse the XML DOM (Document Object Model).\n",
    "    - **Cell Objects**: Unlike CSV (where a number is just bytes), a cell in Excel has value, style, type, formula, etc. `openpyxl` often creates a Python object for every single populated cell to capture this metadata.\n",
    "    - This results in massive memory usage. A 5MB Excel file can easily consume 100MB+ of RAM during processing.\n",
    "\n",
    "3.  **Conversion**:\n",
    "    - Finally, the values are extracted from these heavy Cell objects and placed into NumPy arrays for the Pandas DataFrame, releasing the heavy XML/Cell objects (Garbage Collection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Files - Unstructured Data\n",
    "\n",
    "Text files are unstructured and can contain free-form text.\n",
    "\n",
    "They contain free-form text without a predefined schema, making them harder to analyze directly compared to structured formats like Excel or CSV.\n",
    "\n",
    "Examples: news articles, reviews, emails, logs, social media posts.\n",
    "\n",
    "Text files are considered unstructured because:\n",
    "\n",
    "- No fixed rows and columns\n",
    "- No data types or schema\n",
    "- Content may be sentences, paragraphs, logs, or notes\n",
    "- Meaning is embedded in natural language\n",
    "\n",
    "### Reading Text\n",
    "- Use `open()` with 'r' mode.\n",
    "- Methods: read(), readline(), readlines().\n",
    "\n",
    "### Writing Text\n",
    "- Use `open()` with 'w' or 'a' mode.\n",
    "- Methods: write(), writelines()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file written to: data/example.txt\n"
     ]
    }
   ],
   "source": [
    "# Writing to text file\n",
    "text_file = 'data/example.txt'\n",
    "with open(text_file, 'w') as f:\n",
    "    f.write('This is line 1.\\n')\n",
    "    f.write('This is line 2.\\n')\n",
    "    f.writelines(['Line 3.\\n', 'Line 4.\\n'])\n",
    "print(f'Text file written to: {text_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire content:\n",
      "This is line 1.\n",
      "This is line 2.\n",
      "Line 3.\n",
      "Line 4.\n",
      "\n",
      "\n",
      "Line by line:\n",
      "\n",
      "\n",
      "This is line 1.\n",
      "\n",
      "\n",
      "This is line 2.\n",
      "\n",
      "\n",
      "Line 3.\n",
      "\n",
      "\n",
      "Line 4.\n",
      "\n",
      "Lines list:\n",
      "['This is line 1.\\n', 'This is line 2.\\n', 'Line 3.\\n', 'Line 4.\\n']\n"
     ]
    }
   ],
   "source": [
    "# Reading entire text\n",
    "with open(text_file, 'r') as f:\n",
    "    content = f.read()\n",
    "print('Entire content:')\n",
    "print(content)\n",
    "\n",
    "# Reading line by line\n",
    "print('\\nLine by line:')\n",
    "with open(text_file, 'r') as f:\n",
    "    for line in f:\n",
    "        print('\\n')\n",
    "        print(line.strip())\n",
    "\n",
    "# Reading all lines into list\n",
    "with open(text_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print('\\nLines list:')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Deep Dive: Text Streams & Buffering\n",
    "\n",
    "Handling unstructured text gives you the most control over memory.\n",
    "\n",
    "1.  **File Descriptors & Buffers**:\n",
    "    - `open()` requests a file descriptor from the OS.\n",
    "    - A specific buffer size is allocated (e.g., 4096 bytes).\n",
    "    - When you read, the disk head moves, fills the buffer, and Python reads from RAM (the buffer).\n",
    "\n",
    "2.  **Load vs. Stream**:\n",
    "    - **`read()`**: Loads the **entire** file contents into a single Python string. For a 10GB log file, this will crash your program (MemoryError).\n",
    "    - **`for line in f:`**: This is a **Lazy Iterator**. Python reads just enough bytes to find the next newline logic (`\\n`).\n",
    "    - At any specific moment, only one line is stored in Python's memory. The previous line is discarded (eligible for Garbage Collection).\n",
    "    \n",
    "3.  **Encoding**:\n",
    "    - Files are stored as bytes (0s and 1s).\n",
    "    - `open(..., encoding='utf-8')` applies a decoding layer. It converts raw bytes into **Unicode Code Points** (Python strings).\n",
    "    - This decoding has a small CPU cost but ensures valid character representation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Optionally, remove generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# files_to_remove = ['data/example.csv', 'data/example.json', 'data/example_builtin.json', 'data/nested.json', 'data/example.xlsx', 'data/multi_sheet.xlsx', 'data/example.txt']\n",
    "# for file in files_to_remove:\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)\n",
    "#         print(f'Removed: {file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.15.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
