{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling Large Datasets with Chunking and Lazy Evaluation\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "A large dataset is not defined by file size alone.\n",
        "It is large when it cannot be processed efficiently with naïve, in-memory methods on your system.\n",
        "\n",
        "A dataset is large if it forces you to think about memory, disk I/O, or network constraints explicitly.\n",
        "Large datasets often exceed the available RAM, leading to memory errors. Two key strategies to manage this are:\n",
        "- **Chunking**: Processing data in smaller pieces (chunks).\n",
        "- **Lazy Evaluation**: Delaying computations until necessary, often using libraries like Dask.\n",
        "\n",
        "We'll use Pandas for chunking and Dask for lazy evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chunking with Pandas\n",
        "\n",
        "Pandas allows reading CSV files in chunks using the `chunksize` parameter in `pd.read_csv()`. This returns an iterator of DataFrames.\n",
        "\n",
        "### Why Chunking?\n",
        "- Reduces memory usage.\n",
        "- Enables processing of files larger than memory.\n",
        "\n",
        "### Example: Summing a Large CSV File\n",
        "\n",
        "Assume we have a large CSV file 'large_data.csv' with columns 'A', 'B', 'C'. We'll compute the sum of column 'A' by processing in chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulated data created.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Generate a large CSV file for simulation. It is not large enough to require Dask, but simulates the scenario.\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'A': range(1000000),\n",
        "    'B': range(1000000),\n",
        "    'C': range(1000000)\n",
        "})\n",
        "data.to_csv('large_data.csv', index=False)\n",
        "\n",
        "print('Simulated data created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation\n",
        "\n",
        "- `pd.read_csv(..., chunksize=n)`: Returns TextFileReader iterator.\n",
        "- Loop over chunks: Process each DataFrame chunk individually.\n",
        "- Aggregate results: Here, summing sums.\n",
        "\n",
        "This way, only one chunk is in memory at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Processing chunk with 100000 rows\n",
            "Total sum of column A: 499999500000\n"
          ]
        }
      ],
      "source": [
        "# Reading in chunks\n",
        "chunk_size = 100000  # Adjust based on memory\n",
        "total_sum = 0\n",
        "\n",
        "for chunk in pd.read_csv('large_data.csv', chunksize=chunk_size):\n",
        "    print(f'Processing chunk with {len(chunk)} rows')\n",
        "    total_sum += chunk['A'].sum()\n",
        "\n",
        "print(f'Total sum of column A: {total_sum}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Advanced Chunking: Filtering and Writing\n",
        "\n",
        "Example: Filter rows where 'A' > 500000 and write to a new file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered data written.\n"
          ]
        }
      ],
      "source": [
        "with open('filtered_data.csv', 'w') as f:\n",
        "    header = True\n",
        "    for chunk in pd.read_csv('large_data.csv', chunksize=chunk_size):\n",
        "        filtered = chunk[chunk['A'] > 500000]\n",
        "        filtered.to_csv(f, index=False, header=header)\n",
        "        header = False  # Only write header once\n",
        "\n",
        "print('Filtered data written.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lazy Evaluation with Dask\n",
        "\n",
        "Lazy evaluation is a programming technique where expressions are not computed until their values are needed. Memory is used only for what’s needed right now, not the entire dataset.\n",
        "\n",
        "Python provides several lazy structures: \n",
        "- Iterator : Object with __next__()\n",
        "- Generators : yield is the key i.e, it pauses the function and resumes later.\n",
        "\n",
        "Dask provides parallel computing and lazy evaluation for larger-than-memory datasets. It mimics Pandas API but computes lazily.\n",
        "\n",
        "### Why Lazy Evaluation?\n",
        "- Builds computation graphs without executing immediately.\n",
        "- Optimizes and parallelizes computations.\n",
        "- Handles big data with clusters or multi-core.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation\n",
        "# If not installed:\n",
        "# %pip install dask[complete] \n",
        "# %pip install \"pyarrow>=10.0.1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Dask library work ?\n",
        "\n",
        "- `dd.read_csv()`: Creates a Dask DataFrame, partitioned into smaller Pandas DataFrames.\n",
        "- Operations like `.mean()`, `.sum()` are lazy: They build a task graph.\n",
        "- `dd.compute()`: Executes the graph and returns results.\n",
        "\n",
        "This avoids loading the entire dataset into memory.\n",
        "\n",
        "\n",
        "### Example: Basic Dask DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of A: 499999.5, Sum of B: 499999500000\n"
          ]
        }
      ],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Read the large CSV with Dask\n",
        "ddf = dd.read_csv('large_data.csv')\n",
        "\n",
        "# Lazy operations\n",
        "mean_A = ddf['A'].mean()\n",
        "sum_B = ddf['B'].sum()\n",
        "\n",
        "# Compute only when needed\n",
        "results = dd.compute(mean_A, sum_B)\n",
        "\n",
        "print(f'Mean of A: {results[0]}, Sum of B: {results[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GroupBy and Persist\n",
        "\n",
        "### Persist \n",
        "Persisting means storing intermediate results to a stable storage medium (RAM, disk, or distributed storage) so that it can be reuse them later without recomputing.\n",
        "- `persist()`: Computes and keeps the DataFrame in memory (or disk if specified).\n",
        "- Useful for datasets that fit in distributed memory but not single machine.\n",
        "\n",
        "Example: Group by a category ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category\n",
            "Cat1    499999.0\n",
            "Cat2    500000.0\n",
            "Name: A, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Add category to data for demo\n",
        "data['Category'] = ['Cat1' if i % 2 == 0 else 'Cat2' for i in range(len(data))]\n",
        "data.to_csv('large_data_with_cat.csv', index=False)\n",
        "\n",
        "ddf = dd.read_csv('large_data_with_cat.csv')\n",
        "\n",
        "# Lazy groupby\n",
        "grouped_mean = ddf.groupby('Category')['A'].mean()\n",
        "\n",
        "# Persist in memory for repeated use\n",
        "ddf = ddf.persist()\n",
        "\n",
        "# Compute\n",
        "result = grouped_mean.compute()\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combining Chunking and Lazy Evaluation\n",
        "\n",
        "Dask internally uses chunking. You can control partitions with `npartitions`. `npartitions` refers to the number of smaller, independent chunks (partitions) that a large dataset is split into for parallel processing.\n",
        "\n",
        "If there are too few partitions, parallelism is limited and processing is slower; if there are too many, task overhead increases and performance suffers, so the ideal is usually 2–4 partitions per CPU core.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Read a large CSV lazily\n",
        "df = dd.read_csv('large_data_with_cat.csv')\n",
        "\n",
        "# Check number of partitions\n",
        "print(df.npartitions)\n",
        "# Repartition to 10 partitions\n",
        "df = df.repartition(npartitions=10)\n",
        "\n",
        "print(df.npartitions)  # 10\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
